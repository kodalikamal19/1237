"""
HackRX Scoring Optimizer
Specialized component designed to maximize scoring performance based on HackRX evaluation criteria.
Focus on accuracy for unknown documents with high weightage.
"""

import os
import json
import re
import logging
from typing import List, Dict, Any, Tuple, Optional
import hashlib
from dataclasses import dataclass
from src.training.optimized_query_processor import OptimizedQueryProcessor
from src.training.faiss_semantic_search import AdvancedFAISSSearch
from src.utils.memory_manager import MemoryManager

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScoringStrategy:
    """Represents a scoring strategy with metadata"""
    name: str
    description: str
    weight_multiplier: float
    confidence_threshold: float
    fallback_strategy: Optional[str] = None

class HackRXScoringOptimizer:
    """
    Scoring optimizer specifically designed for HackRX evaluation criteria.
    Maximizes performance on unknown documents which have higher weightage.
    """
    
    def __init__(self, query_processor: OptimizedQueryProcessor):
        self.query_processor = query_processor
        self.memory_manager = MemoryManager()
        
        # HackRX scoring parameters (based on documentation)
        self.document_weights = {
            'known': 0.5,      # Lower weightage for known documents
            'unknown': 2.0     # Higher weightage for unknown documents
        }
        
        # Question complexity weights (estimated)
        self.question_weights = {
            'simple': 1.0,     # Basic factual questions
            'moderate': 1.5,   # Questions requiring interpretation
            'complex': 2.0     # Multi-step reasoning questions
        }
        
        # Scoring strategies for different scenarios
        self.scoring_strategies = {
            'unknown_document': ScoringStrategy(
                name="Unknown Document Strategy",
                description="Optimized for high-value unknown documents",
                weight_multiplier=2.0,
                confidence_threshold=0.8,
                fallback_strategy='conservative_fallback'
            ),
            'known_document': ScoringStrategy(
                name="Known Document Strategy", 
                description="Standard processing for known documents",
                weight_multiplier=0.5,
                confidence_threshold=0.6,
                fallback_strategy='aggressive_fallback'
            ),
            'conservative_fallback': ScoringStrategy(
                name="Conservative Fallback",
                description="High accuracy fallback for critical questions",
                weight_multiplier=1.0,
                confidence_threshold=0.9,
                fallback_strategy=None
            ),
            'aggressive_fallback': ScoringStrategy(
                name="Aggressive Fallback",
                description="Fast processing for lower weight questions",
                weight_multiplier=1.0,
                confidence_threshold=0.4,
                fallback_strategy=None
            )
        }
        
        # Document classification patterns
        self.document_classifiers = {
            'insurance_patterns': [
                r'national\s+parivar\s+mediclaim',
                r'policy\s+number',
                r'sum\s+insured',
                r'premium\s+payment',
                r'grace\s+period',
                r'waiting\s+period',
                r'pre-existing\s+diseases?',
                r'maternity\s+expenses?',
                r'no\s+claim\s+discount',
                r'ayush\s+treatment',
                r'organ\s+donor',
                r'room\s+rent\s+capping'
            ],
            'legal_patterns': [
                r'agreement\s+between',
                r'terms\s+and\s+conditions',
                r'liability\s+clause',
                r'indemnification',
                r'breach\s+of\s+contract',
                r'force\s+majeure'
            ],
            'hr_patterns': [
                r'employee\s+handbook',
                r'compensation\s+structure',
                r'performance\s+review',
                r'leave\s+policy',
                r'code\s+of\s+conduct'
            ]
        }
        
        # Question complexity classifiers
        self.complexity_patterns = {
            'simple': [
                r'^what\s+is\s+the\s+\w+',
                r'^how\s+much\s+is\s+the\s+\w+',
                r'^when\s+is\s+the\s+\w+',
                r'^\w+\s+days?\?',
                r'^\w+\s+months?\?',
                r'^\w+\s+years?\?'
            ],
            'moderate': [
                r'^does\s+.*\s+cover\s+.*\?',
                r'^what\s+are\s+the\s+conditions\s+.*\?',
                r'^how\s+does\s+.*\s+work\s+.*\?',
                r'^what\s+is\s+the\s+process\s+.*\?'
            ],
            'complex': [
                r'^.*\s+and\s+what\s+are\s+.*\?',
                r'^.*\s+including\s+.*\s+and\s+.*\?',
                r'^under\s+what\s+circumstances\s+.*\?',
                r'^.*\s+provided\s+that\s+.*\?'
            ]
        }
    
    def classify_document_type(self, document_text: str) -> str:
        """
        Classify if document is likely to be known or unknown based on patterns.
        Unknown documents get higher scoring weight.
        """
        text_lower = document_text.lower()
        
        # Check for known document patterns (common insurance policy indicators)
        known_indicators = 0
        for pattern_category, patterns in self.document_classifiers.items():
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    known_indicators += 1
        
        # Heuristic: if document matches many patterns, likely known
        if known_indicators >= 5:
            return 'known'
        else:
            return 'unknown'  # Default to unknown for higher scoring potential
    
    def classify_question_complexity(self, question: str) -> str:
        """Classify question complexity for appropriate weight assignment"""
        question_lower = question.lower()
        
        for complexity, patterns in self.complexity_patterns.items():
            for pattern in patterns:
                if re.search(pattern, question_lower):
                    return complexity
        
        return 'moderate'  # Default to moderate complexity
    
    def calculate_expected_score(self, document_type: str, question_complexity: str) -> float:
        """Calculate expected score contribution for a question"""
        doc_weight = self.document_weights.get(document_type, 1.0)
        question_weight = self.question_weights.get(question_complexity, 1.0)
        
        return doc_weight * question_weight
    
    def optimize_processing_strategy(self, document_text: str, questions: List[str]) -> Dict[str, Any]:
        """
        Optimize processing strategy based on document type and question complexity.
        Returns strategy recommendations for maximum scoring.
        """
        
        # Classify document
        document_type = self.classify_document_type(document_text)
        
        # Analyze questions
        question_analysis = []
        total_expected_score = 0
        
        for i, question in enumerate(questions):
            complexity = self.classify_question_complexity(question)
            expected_score = self.calculate_expected_score(document_type, complexity)
            
            question_analysis.append({
                'index': i,
                'question': question,
                'complexity': complexity,
                'expected_score': expected_score,
                'priority': 'high' if expected_score > 2.0 else 'medium' if expected_score > 1.0 else 'low'
            })
            
            total_expected_score += expected_score
        
        # Sort questions by expected score (prioritize high-value questions)
        question_analysis.sort(key=lambda x: x['expected_score'], reverse=True)
        
        # Select processing strategy
        strategy_name = f"{document_type}_document"
        strategy = self.scoring_strategies[strategy_name]
        
        optimization_plan = {
            'document_type': document_type,
            'document_weight': self.document_weights[document_type],
            'total_expected_score': total_expected_score,
            'processing_strategy': strategy,
            'question_analysis': question_analysis,
            'high_priority_questions': [q for q in question_analysis if q['priority'] == 'high'],
            'recommendations': self._generate_recommendations(document_type, question_analysis)
        }
        
        logger.info(f"Document classified as: {document_type}")
        logger.info(f"Total expected score: {total_expected_score:.2f}")
        logger.info(f"High priority questions: {len(optimization_plan['high_priority_questions'])}")
        
        return optimization_plan
    
    def _generate_recommendations(self, document_type: str, question_analysis: List[Dict]) -> List[str]:
        """Generate specific recommendations for optimization"""
        recommendations = []
        
        high_value_count = sum(1 for q in question_analysis if q['expected_score'] > 2.0)
        
        if document_type == 'unknown':
            recommendations.extend([
                "ðŸ“ˆ HIGH VALUE DOCUMENT: Focus on accuracy over speed",
                "ðŸŽ¯ Use enhanced semantic search for better clause matching",
                "ðŸ” Apply conservative answer validation",
                "âš¡ Prioritize API processing over fallback methods",
                f"ðŸ’Ž {high_value_count} high-value questions identified"
            ])
        else:
            recommendations.extend([
                "âš¡ KNOWN DOCUMENT: Balance speed and accuracy",
                "ðŸš€ Use aggressive caching strategies", 
                "ðŸ”„ Fallback methods acceptable for lower priority questions",
                f"ðŸ“Š {high_value_count} high-value questions identified"
            ])
        
        if high_value_count > len(question_analysis) * 0.5:
            recommendations.append("ðŸŽ–ï¸ CRITICAL: >50% questions are high-value - maximize accuracy")
        
        return recommendations
    
    def process_with_scoring_optimization(self, document_text: str, questions: List[str]) -> List[str]:
        """
        Process queries with HackRX scoring optimization.
        Implements prioritized processing for maximum score.
        """
        
        # Get optimization plan
        optimization_plan = self.optimize_processing_strategy(document_text, questions)
        
        strategy = optimization_plan['processing_strategy']
        question_analysis = optimization_plan['question_analysis']
        
        logger.info(f"Using strategy: {strategy.name}")
        
        # Process questions in priority order
        results = {}
        processed_count = 0
        
        for question_info in question_analysis:
            question = question_info['question']
            expected_score = question_info['expected_score']
            index = question_info['index']
            
            try:
                # Use different processing approaches based on priority
                if question_info['priority'] == 'high':
                    # High priority: Use best available method
                    result = self._process_high_priority_question(
                        document_text, question, strategy
                    )
                elif question_info['priority'] == 'medium':
                    # Medium priority: Standard processing
                    result = self._process_medium_priority_question(
                        document_text, question, strategy
                    )
                else:
                    # Low priority: Fast processing
                    result = self._process_low_priority_question(
                        document_text, question, strategy
                    )
                
                results[index] = result.answer if hasattr(result, 'answer') else result
                processed_count += 1
                
                logger.info(f"Processed question {processed_count}/{len(questions)} "
                          f"(priority: {question_info['priority']}, "
                          f"expected_score: {expected_score:.2f})")
                
            except Exception as e:
                logger.error(f"Error processing question {index}: {str(e)}")
                results[index] = f"Error processing question: {str(e)}"
        
        # Reorder results to match original question order
        final_answers = []
        for i in range(len(questions)):
            final_answers.append(results.get(i, "Error: Question not processed"))
        
        logger.info("Scoring-optimized processing completed")
        return final_answers
    
    def _process_high_priority_question(self, document_text: str, question: str, strategy: ScoringStrategy):
        """Process high-priority questions with maximum accuracy"""
        
        # Use the most accurate method available
        if hasattr(self.query_processor, 'process_query_optimized'):
            result = self.query_processor.process_query_optimized(document_text, question)
            
            # Validate result quality
            if (hasattr(result, 'confidence_score') and 
                result.confidence_score >= strategy.confidence_threshold):
                return result
        
        # If confidence is low, try fallback strategy
        if strategy.fallback_strategy:
            fallback_strategy = self.scoring_strategies[strategy.fallback_strategy]
            return self._process_with_fallback(document_text, question, fallback_strategy)
        
        # Final fallback
        return self.query_processor.process_query_hybrid(document_text, question)
    
    def _process_medium_priority_question(self, document_text: str, question: str, strategy: ScoringStrategy):
        """Process medium-priority questions with balanced approach"""
        
        if hasattr(self.query_processor, 'process_query_optimized'):
            return self.query_processor.process_query_optimized(document_text, question)
        else:
            return self.query_processor.process_query_hybrid(document_text, question)
    
    def _process_low_priority_question(self, document_text: str, question: str, strategy: ScoringStrategy):
        """Process low-priority questions with emphasis on speed"""
        
        # Try fastest method first
        try:
            return self.query_processor.process_query_hybrid(document_text, question)
        except Exception as e:
            # If fast method fails, use standard processing
            logger.warning(f"Fast processing failed, using standard: {str(e)}")
            if hasattr(self.query_processor, 'process_query_optimized'):
                return self.query_processor.process_query_optimized(document_text, question)
            else:
                return f"Error processing question: {str(e)}"
    
    def _process_with_fallback(self, document_text: str, question: str, fallback_strategy: ScoringStrategy):
        """Process with specific fallback strategy"""
        
        if fallback_strategy.name == "Conservative Fallback":
            # Use most conservative/accurate method
            if hasattr(self.query_processor, 'semantic_search'):
                # Use semantic search for better clause matching
                relevant_clauses = self.query_processor.semantic_search.find_relevant_clauses(
                    question, document_text
                )
                
                # Use enhanced context for processing
                enhanced_context = "\n".join([clause['content'] for clause in relevant_clauses[:3]])
                
                if hasattr(self.query_processor, 'process_query_optimized'):
                    return self.query_processor.process_query_optimized(enhanced_context, question)
        
        # Default fallback
        return self.query_processor.process_query_hybrid(document_text, question)
    
    def get_scoring_analysis(self, document_text: str, questions: List[str], answers: List[str]) -> Dict[str, Any]:
        """
        Analyze the scoring potential of the results.
        Provides insights for further optimization.
        """
        
        optimization_plan = self.optimize_processing_strategy(document_text, questions)
        
        # Calculate theoretical maximum score
        max_possible_score = sum(q['expected_score'] for q in optimization_plan['question_analysis'])
        
        # Estimate achieved score (simplified heuristic)
        estimated_score = 0
        for i, (question_info, answer) in enumerate(zip(optimization_plan['question_analysis'], answers)):
            if answer and len(answer.strip()) > 10 and "Error" not in answer:
                # Assume correct answer for non-empty, non-error responses
                estimated_score += question_info['expected_score']
        
        scoring_analysis = {
            'document_type': optimization_plan['document_type'],
            'document_weight': optimization_plan['document_weight'],
            'total_questions': len(questions),
            'high_priority_questions': len(optimization_plan['high_priority_questions']),
            'max_possible_score': max_possible_score,
            'estimated_achieved_score': estimated_score,
            'score_efficiency': estimated_score / max_possible_score if max_possible_score > 0 else 0,
            'recommendations_followed': len(optimization_plan['recommendations']),
            'optimization_summary': {
                'strategy_used': optimization_plan['processing_strategy'].name,
                'prioritization_applied': True,
                'high_value_focus': len(optimization_plan['high_priority_questions']) > 0
            }
        }
        
        logger.info(f"Scoring Analysis: {estimated_score:.2f}/{max_possible_score:.2f} "
                   f"({scoring_analysis['score_efficiency']:.1%} efficiency)")
        
        return scoring_analysis
